{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPFL-MICRO452 - Mobile Robotics Project\n",
    "Project for the Mobile Robotic course, fall semester 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "**Rocca Federico** (390233) - First year MSc in Robotics at EPFL, previous degree BSc in Computer Engineering at Politecnico di Milano  \n",
    "**Rashidi Mohammad Massi** (394309) - First year MSc in CyberSecurity at EPFL previous degree BSc in Computer Science at University of Geneva       \n",
    "**Rawas Mouhamad Bilal** (345489) - First year MSc in Robotics at EPFL, previous degree BSc in Microengineering at EPFL    \n",
    "**Schär Mikaël Joël Michel** (325388) - First year MSc in Robotics at EPFL, previous degree BSc in Microengineering at EPFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of the project is to build a system able of controlling a <a href=\"https://www.thymio.org/\" target=\"_blank\">Thymio</a> robot. This system shoud integrate all the main modules that are tipically found in a simple mobile robot:\n",
    "- **Vision**\n",
    "- **Global Navigation**\n",
    "- **Local Navigation**\n",
    "- **Filtering**\n",
    "\n",
    "The robot should will be placed in an **environment** (explained later) where there are some **permanent obstacles**. Given a **camera view** of the environment, a **global plan** should be generated, starting from the Thymio's position, leading to the goal and avoiding the permanent obstacles. Some **random obstacles** may be introduced in the environment during the movement of the robot from start to goal, and it should be able to avoid them. The robot should be localized both using the camera image, that can be obstructed on purpose from time to time, and the odometry, by fusing them with the **filtering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "Here's a demo of the system working:  \n",
    "<p align=\"center\">\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"images/video.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "The environment that the robot has to navigate has been designed by us, and it consists of a **white floor**, the traversable space, where we placed some **black cutouts**, that represent permanent obstacles, while the goal position is indicated by a **red mark** placed on the floor.  \n",
    "<p align=\"center\">\n",
    "    <img src=\"images/original_image.jpeg\" width=\"400\">\n",
    "</P>\n",
    "In order to detect the white Thymio robot in the white background we decided to place an <a href=\"https://april.eecs.umich.edu/software/apriltag\" target=\"_blank\">AprilTag</a> on its top. It is used both for detecting the robot's position and orientation.\n",
    "We also attached 4 AprilTags on the 4 corners of the environment in order to straighten the image that the camera records and generate precise measurements based on that image. The Apriltag's method is inspired by this <a href=\"https://github.com/adrienohana/thymioproject/blob/main/computer_vision.py\" target=\"_blank\">other project</a>.\n",
    "<!-- Tag du robot -->\n",
    "<p align=\"center\">\n",
    "    <span style=\"display: block; font-weight: bold; margin-bottom: 10px;\">Tag corresponding to the robot</span>\n",
    "    <img src=\"images/tag.png\" width=\"200\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Tags des 4 coins -->\n",
    "<p align=\"center\">\n",
    "    <span style=\"display: block; font-weight: bold; margin-bottom: 10px;\">Tags corresponding to the 4 corners of the image</span>\n",
    "    <img src=\"images/tagStandard41h12-100.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/tagStandard41h12-113.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/tagStandard41h12-141.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/tagStandard41h12-283.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Implemented\n",
    "- The **vision** module generates the map of the environment, starting from a camera image.\n",
    "- The **vision** module also tracks the pose of the robot.\n",
    "- The information from the **vision** is also used to detect and solve any kidnapping situation, by relocalizing the robot in the map\n",
    "- The **global navigation** module, given the map of the environment, with a starting position and a goal, deigns the optimal plan using the Dijkstra algorithm.\n",
    "- The Thymio control module generates the movement commands that allow the robot to move along the path from start to finish\n",
    "- The **local navigation** module is used for avoiding the unexpected obstacles that might be detected along the path\n",
    "- The **filtering** module implements a Kalman filter in order to better localize the robot. It predicts the robots position given the movements made and updates the prediction using data from the vision.\n",
    "- The **filtering** is also used for having an estimate of the position when the camera is covered and allow the Thymio to keep moving throwards the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Implemented Modules\n",
    "The next sections will explain how each module is implemented and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision**\n",
    "\n",
    "The Vision system detects the 4 corners of the map using AprilTags to align and straighten the image, identifies the robot’s position and orientation with a specific tag, locates the goal by detecting red zones, and generates a matrix that represents the map by highlighting obstacles, which correspond to the black areas on the map, and the accessible areas available for the Thymio robot to navigate. The system calculates and returns the map scale in pixels per centimeter for precise navigation by the Thymio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Demo of Vision System</h2>\n",
    "\n",
    "<!-- Step 1: Original Image -->\n",
    "<p align=\"center\">\n",
    "    <strong>1. Original Image Captured</strong><br>\n",
    "    <img src=\"images/original_image.jpeg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 2: Cropped Image -->\n",
    "<p align=\"center\">\n",
    "    <strong>2. Cropped and Aligned Image</strong><br>\n",
    "    <img src=\"images/cropped_image.jpeg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 3: Image with Start and Goal Highlighted -->\n",
    "<p align=\"center\">\n",
    "    <strong>3. Image with the center of the Start and the Goal Highlighted</strong><br>\n",
    "    <img src=\"images/start_goal_highlighted.jpg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 4: Image with Red Regions and Tags Whitened -->\n",
    "<p align=\"center\">\n",
    "    <strong>4. Image with Red Regions and Robot's Tags Whitened</strong><br>\n",
    "    <img src=\"images/whitened_regions.jpg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 5: Final Map with Obstacles Highlighted -->\n",
    "<p align=\"center\">\n",
    "    <strong>5. Final Binary Map Showing Obstacles</strong><br>\n",
    "    <img src=\"images/binary_obstacle_map.jpg\" width=\"500\" style=\"margin: 7px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Planning**\n",
    "The goal of this project is guide the robot from a start position to a goal position. To make this happen, it is necessary to implement a **global navigation** module, implemented in the <code>GlobalPlanning</code> class in <code>globalPlanning.py</code>. It works by building a path that our Thymio will follow, from the position of the robot to the position of our goal placed on our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dijkstra's Algorithm**\n",
    "This algorithm is implemented on a grid representation of the map. Most of the code has been taken from the serie 5 of the course. In this grid it will search the most efficient path to reach our goal. Here an exemple of a grid taken from exercice session 5:  \n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Grid with the start \"S\" and the goal \"G\"</strong><br>\n",
    "    <img src=\"images/Dijkstra 1.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "The first step is to number all the cell of the map from start to goal. To do this at each step from the start, we take all the neighbors and assign cells one unit larger than the previous one. An Exemple is provide for better understanding :\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Grid with the first iterations of the algorithm</strong><br>\n",
    "    <img src=\"images/Dijkstra 2.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "The second step is to find the most optimized path. To find it, we take our grid and start from the goal. We take all the neighbors and we assign the cell with the smallest number as part of the path. Here an exemple of the shortest path between start and goal.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Grid with the most optimized path in gray</strong><br>\n",
    "    <img src=\"images/Dijkstra 3.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "In our code, we make this algorithm with the 8-connected grid, so we also take into account the diagonals of the current cell's neighbors. We can see the difference between the example here and how we doing in our code.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>4 and 8-connected grid</strong><br>\n",
    "    <img src=\"images/4-connected grid.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/8-connected grid.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "### **Shadows and magnification**\n",
    "\n",
    "Before we call the Dijkstra's algorithm, we need to do somes adjustments on the map to be sure that the Thymio will find a path and won't drive on the obstacles. First step, we remove the \"obstacles\" around the thymio that could have been created by its own shadow or by its wheels. Second step, we proceed to enlarge the obstacles to avoid the path to come too close to the, and the robot to drive over an obstacle due to its width. Here you can see a result of the map before and after the magnification:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Map before and after magnification</strong><br>\n",
    "    <img src=\"images/before magnification.png\" style=\"width: 45%;\">\n",
    "    <img src=\"images/after magnification.png\" style=\"width: 45%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Planning**\n",
    "Once the global navigation module has designed a path from the start position to the goal, the local planning module has to generate the movement commands for the robot in such a way that it will follow that path, all while avoiding unexpected obstacles that might be detected while moving from the start position to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motion Control**\n",
    "The motion control, that is the process of generating motion commands to get the robot to follow the path generated by the global plannign module, is implemented in the <code>ThymioControl</code> class in <code>thymioControl.py</code>\n",
    "\n",
    "#### **Path Reduction**\n",
    "As a first step, it reduces the path to the minumum sequence of waypoints. This means that all the points in the path that lay on the same straight line, excluding the first and last, are removed. Here is shown an example of this behavior.  \n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"images/ogPath.png\" alt=\"Original path\" style=\"width: 45%;\"/>\n",
    "  <img src=\"images/reducedPath.png\" alt=\"Reduced path\" style=\"width: 45%;\"/>\n",
    "</div>\n",
    "\n",
    "#### **Kidnapping Detection**\n",
    "This module is also used for recognizing kidnapping situations. This means that when a new measurement of position and angle is received from the camera, it is compared with the prediction of position and angle produced by the Kalman filter at the previous step, and if the position or the angle differ by more than the relative thresholds <code>self.__kidnappingThresholdPosition</code> and <code>self.__kidnappingThresholdAngle</code>, a kidnapping situation is signaled and the kidnapping routine, explained later, is run.\n",
    "\n",
    "#### **Movement Generation**\n",
    "Given the current position and angle of the Thymio and the position of the next waypoint in the path that needs to be reached, the required velocity commands are calculated in this module. Two different approaches are present in the file: <code>move_pd(position, angle)</code> or <code>move(position, angle)</code>\n",
    "- <code>move_pd</code>: the linear speed is considered always constant, while the angular speed is calculated using a PD controller, based on the angle difference between the Thymio orientation and the relative angle of the waypoint in respect to the robot for the proportional part and on the previous angle difference for the derivative part. The linear and angular speed are then used to compute the left and right motor speeds using differential drive kinematics.  \n",
    "  PD controller: $$w = k_p \\cdot \\text{angleDistance} + k_d \\cdot \\frac{\\text{angleDistance} - \\text{previousAngleDistance}}{dt}$$\n",
    "  with $k_p$ and $k_d$ respectively the proportional and derivative gain.\n",
    "- <code>move</code>: this time, when an angle difference is detected between the Thymio's orientation and the direction throwards the waypoint, the robot is stopped (linear speed = 0), and it is rotated on the spot. The angular velocity used for turning is once again calculated using a PD controller. If instead the angle difference is less then the threshold <code>self.__angleThreshold</code>, the angular velocity is set to 0 and the linear velocity is set to a predetermined constant value, making the Thymio go straight throwards the waypoint.\n",
    "Both methods check if the current waypoint has been reached, by computing the distance from the robot to the waypoint and checking if it is less than the threshold <code>self.__reachedThreshold</code>, and if the waypoint has been overshot, maybe during a phase of obstacle avoidance, by comparing the distances between the robot and the next waypoint and betweent the current waypoint and the next wayopint; in both cases it would move onto the next waypoint.  \n",
    "While the first method never stops the robot, so it is potentially faster and smoother, tuning the parameters $k_p$ and $k_d$ wrongly may lead to overshoots or oscillations. On the other hand, the stop-rotate-go controller is more precise but obviously slower, since the robot has to stop before turning on the spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Avoidance\n",
    "An important part in robot's navigation is the avoidance of unexpected obstacles that might be detected while moving from the start position to the goal, following the global plan. The process of local avoidance consists of detecting the obstacles using the robots sensors and designing a more or less efficient plan to get around it, avoiding collisions, and get back on the predetermined global plan.\n",
    "The python file <code>localPlanning.py</code> contains the <code>LocalPlanning</code> class that implements the local avoidance modules.\n",
    "\n",
    "#### Obstacle Detection\n",
    "When the <code>self.is_obstacle_avoidance(prox_horizontal)</code> method is called, a boolean value is returned, <code>True</code> if any of the sensors is reading higher than the threshold, <code>False</code> otherwise.\n",
    "\n",
    "#### Obstacle Avoidance\n",
    "If an obstacle is detected, the obstacle avoidance routine <code>self.obstacle_avoidance(prox_horizontal)</code> generates directly the motor commands **wl** and **wr** by multiplying the values recorded by the proximity sensors by two symmetric arrays of weights, one for each motor, virtually generating a push, away from the obstacle. This implements a really simple neural network, with one neuron for each motor, that takes the sensor values as inputs and computes the motor command. This approach is inspired by what was proposed in exercise session 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kalman Filter**\n",
    "\n",
    "The Kalman Filter is a mathematical tool used in this project to estimate the robot's position and orientation (state) while navigating toward a goal. The robot uses data from two sources: **odometry** (wheel speeds) for prediction and a **camera** for occasional updates. Since both these sources are noisy and prone to errors, the Kalman Filter combines their information to produce an accurate and smooth estimate of the robot’s state. It is implemented in the <code>Kalman</code> class in <code>kalman.py</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Nonlinear Motion Model**\n",
    "\n",
    "A differential-drive robot's motion is inherently nonlinear because its position $(x, y)$ and orientation $\\theta$ depend on trigonometric relationships. For example, its state evolves as (motion model):\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t + v \\cos(\\theta) \\Delta t, \\quad y_{t+1} = y_t + v \\sin(\\theta) \\Delta t, \\quad \\theta_{t+1} = \\theta_t + \\omega \\Delta t\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $v$: Linear velocity (derived from wheel speeds).\n",
    "- $\\omega$: Angular velocity (based on differential wheel motion).\n",
    "- $\\Delta t$: Time step.\n",
    "\n",
    "The EKF linearizes these equations using the Jacobian of the motion model. In the **Kalman class**, the motion model is embedded in the matrix $\\mathbf{G}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fusing Odometry and Camera Data**\n",
    "\n",
    "Odometry (wheel encoder data) is prone to errors such as:\n",
    "\n",
    "- **Drift** over time, accumulating inaccuracies as the robot moves.\n",
    "- **Wheel slippage** and uneven terrain causing deviations from the true trajectory.\n",
    "\n",
    "The camera, while more accurate, provides intermittent data due to potential obstructions or missed detections. The EKF combines these two sources of data to provide a reliable state estimate:\n",
    "\n",
    "1. **Prediction Step**: This step uses the odometry data to predict the robot's next state. It always runs, regardless of whether camera data is available, ensuring continuity in state estimation. The prediction compensates for missing measurements and accounts for process noise.\n",
    "\n",
    "2. **Update Step**: This step completes the prediction using camera measurements when available. It refines the estimated state by fusing the relatively accurate camera data with the prediction. The update step only runs if the camera is not obstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How the Kalman Filter Works**\n",
    "\n",
    "**A. Prediction Step**\n",
    "\n",
    "Using the robot's last known state, wheel speeds, and time elapsed (Δt), the Kalman Filter predicts the robot's next state:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{pred}} = \\mathbf{A} \\cdot \\mathbf{E} + \\mathbf{G} \\cdot \\mathbf{U}\n",
    "$$\n",
    "\n",
    "- **A**: State transition matrix (accounts for constant movement without changes).\n",
    "- **B**: Control matrix (maps wheel speeds to motion).\n",
    "- **U**: Control vector (wheel speeds).\n",
    "\n",
    "It also predicts the **uncertainty** in the state, denoted by the covariance matrix **P**:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{\\text{pred}} = \\mathbf{A} \\cdot \\mathbf{P} \\cdot \\mathbf{A}^T + \\mathbf{R}\n",
    "$$\n",
    "\n",
    "- **Q**: Process noise covariance (uncertainty from wheel encoders).\n",
    "\n",
    "**B. Update Step**\n",
    "\n",
    "When the camera provides a measurement **Z**, the Kalman Filter updates the state and uncertainty:\n",
    "\n",
    "1. Compute the Kalman gain **K**, which determines how much weight to give to the measurement:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\mathbf{P}_{\\text{pred}} \\cdot \\mathbf{H}^T \\cdot \\left(\\mathbf{H} \\cdot \\mathbf{P}_{\\text{pred}} \\cdot \\mathbf{H}^T + \\mathbf{Q}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "- **H**: Measurement matrix (maps state to camera readings).\n",
    "- **R**: Measurement noise covariance (uncertainty in camera data).\n",
    "\n",
    "2. Update the state using the measurement:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{update}} = \\mathbf{E}_{\\text{pred}} + \\mathbf{K} \\cdot \\left(\\mathbf{Z} - \\mathbf{H} \\cdot \\mathbf{E}_{\\text{pred}}\\right)\n",
    "$$\n",
    "\n",
    "3. Update the uncertainty:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{\\text{update}} = (\\mathbf{I} - \\mathbf{K} \\cdot \\mathbf{H}) \\cdot \\mathbf{P}_{\\text{pred}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Behavior in the Control Loop**\n",
    "\n",
    "1. **Prediction Always Happens**:\n",
    "    * The prediction step runs at every iteration using odometry.\n",
    "    * This ensures the robot always has an estimated position, even if the camera data is unavailable.\n",
    "\n",
    "2. **Update Happens When the Camera Sees the Robot**:\n",
    "    * If the camera detects the robot, the Kalman Filter corrects its prediction using the more accurate camera measurement.\n",
    "\n",
    "3. **Handling Obstructions**:\n",
    "    * If the camera is obstructed, the filter relies solely on the prediction step until the camera resumes detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of Parameters\n",
    "The following section explains how the various parameters used throughout the project where selected and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Obstacle Detection Threshold**: the Thymio robot features 5 horizontal **proximity sensors** it its front part (see Thyimio cheat sheet snippet) that can be used to detect obstacles using infrared technology. The range of values that the sensors return is [0, ~4300] and the updates come at a frequency of 10Hz.\n",
    "    <p align=\"center\">\n",
    "        <img src=\"images/thymio_cheat_sheet1.png\" width=\"800\">\n",
    "    </p>\n",
    "    It is possible to map the readings from the proximity sensors to the real world distance measurements in order to tune the threshold for activating the local avoidance routine. In order to do so, many measurements were taken with an obstacle at a known distance and the average of the values was computed:\n",
    "\n",
    "    | Distance | Sensor Reading (average value) |\n",
    "    | --- | ---: |\n",
    "    | 5 | 4300 |\n",
    "    | 10 | 3095 |\n",
    "    | 15 | 2350 |\n",
    "    | 20 | 1690 |\n",
    "\n",
    "    We chose the vector <code>[2500, 2400, 2300, 2400, 2500]</code> for the thresholds relative to each front sensor, meaning a distance of around 15 cm. Detection close to the ceter has a lower threshold with respect to an obstacle on the side because it is necessary to start avoiding before if the obstacle is located straight in frony of the robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Parameters**\n",
    "\n",
    "1. **Conversion ratio from Thymio speed (PWM) to mm/s**:  \n",
    "   This was calculated by measuring the distance covered over time using ground sensor peaks. The Thymio's speed in mm/s was divided by the corresponding PWM value to determine the conversion ratio.\n",
    "\n",
    "2. **Variance of speed in mm²/s²**:  \n",
    "   The speed variance was computed as the variance of the recorded speed data (in mm/s) after converting using the conversion ratio and excluding initial transients. The variance was split equally between the **speed state** variance and the **speed measurement** variance, each being half of the total variance.   \n",
    "\n",
    "3. **Motor speed adjustment scalar**:  \n",
    "   A scalar adjustment was applied to one motor’s PWM to correct for a speed imbalance between the left and right motors. This value was determined empirically through testing to ensure straight-line movement.\n",
    "\n",
    "4. **Magnification**:\n",
    "   The value is calculated with the Thymio size and convert in cell's format. It is used to remove the shadow around the robot and to enlarge the obstacles in the map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "The next section presents the runnable cells that allow the system to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First step, all the classes that have been explained above need to be imported.\n",
    "In order to be sure that all the python packages are correctly installed and ready to use, it is suggested to run <code>pip install -r requirements.txt</code> to get all of them. It should be noted that the <code>pupil-apriltags</code> package may give some problems with python versions higer than 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the classes in the package\n",
    "from vision import Vision\n",
    "from globalPlanning import GlobalPlanning\n",
    "from thymioControl import ThymioControl\n",
    "from localPlanning import LocalPlanning\n",
    "from kalman import Kalman\n",
    "from plotter import Plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install the <code>tdmclient</code> package, used for connecting and comunicating with the Thymio. The control from the notebook is made possible by the import of <code>tdmclient.notebook</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tdmclient\n",
    "!pip3 install tdmclient --upgrade\n",
    "\n",
    "# import the TDMClient module\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TdmClient Functions\n",
    "The following functions are used for modifying the variables in the Thymio, by comunicating through the tdmclient.\n",
    "Specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "def motor_go(left,right):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = left\n",
    "    motor_right_target = right\n",
    "    \n",
    "@tdmclient.notebook.sync_var\n",
    "def motor_stop():\n",
    "    global motor_left_target,motor_right_target\n",
    "    motor_left_target = 0\n",
    "    motor_right_target = 0\n",
    "    \n",
    "@tdmclient.notebook.sync_var\n",
    "def sensor_data():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal.copy()\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_off():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 0, 0]\n",
    "    leds_bottom_left = [0, 0, 0]\n",
    "    leds_bottom_right = [0, 0, 0]\n",
    "    leds_circle = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "def leds_go_crazy():\n",
    "    for i in range(8):\n",
    "        leds_1()\n",
    "        time.sleep(0.1)\n",
    "        leds_2()\n",
    "        time.sleep(0.1)\n",
    "        leds_3()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_1():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [32, 0, 0]\n",
    "    leds_bottom_left = [0, 32, 0]\n",
    "    leds_bottom_right = [0, 0, 32]\n",
    "    leds_circle = [32, 32, 32, 32, 0, 0, 0, 0]\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_2():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 32, 0]\n",
    "    leds_bottom_left = [0, 0, 32]\n",
    "    leds_bottom_right = [32, 0, 0]\n",
    "    leds_circle = [0, 0, 0, 0, 32, 32, 32, 32]\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_3():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 0, 32]\n",
    "    leds_bottom_left = [32, 0, 0]\n",
    "    leds_bottom_right = [0, 32, 0]\n",
    "    leds_circle = [32, 0, 32, 0, 32, 0, 32, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Loop\n",
    "The next code section presents the main control loop that unites all the modules and functionalities. It is implemented following the finite state machine here below:\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/thymioStateDiagram.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leds off during the navigation\n",
    "leds_off()\n",
    "\n",
    "# initialize objects\n",
    "globalPlanning = GlobalPlanning()\n",
    "localPlanning = LocalPlanning()\n",
    "filter = Kalman()\n",
    "thymio = ThymioControl()\n",
    "plotter = Plotter()\n",
    "\n",
    "# initialize variables for saving the trajectory and the Kalman filter positions\n",
    "row_trajectory = []\n",
    "col_trajectory = []\n",
    "row_kalman_pred = []\n",
    "col_kalman_pred = []\n",
    "\n",
    "# timeout for the loop\n",
    "timeout = 0.1\n",
    "# delta time between iterations, considering all the computation time\n",
    "dt = 0\n",
    "# old time to compute the delta time\n",
    "old_time = time.time_ns()\n",
    "\n",
    "# create a vision object\n",
    "image_path = \"images/original_image.jpeg\"\n",
    "vision = Vision(fps=3, threshold=50, target_height=80, default_image_path=image_path)\n",
    "\n",
    "# iteration counter\n",
    "iter = 1\n",
    "\n",
    "# variables for position and angle\n",
    "position = None\n",
    "angle = None\n",
    "\n",
    "# flag to check if the goal is reached\n",
    "goal = False\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# initialization from vision\n",
    "vision.update_image()\n",
    "# getting the initial position and angle from the camera\n",
    "pos_vision = vision.getStart()\n",
    "angle_vision = vision.getAngle()\n",
    "i = 10\n",
    "while pos_vision is None and i > 0:\n",
    "    vision.update_image()\n",
    "    pos_vision = vision.getStart()\n",
    "    angle_vision = vision.getAngle()\n",
    "    i -= 1\n",
    "if pos_vision is None:\n",
    "    print(\"MAIN: ERROR: camera obstructed in the first iteration\")\n",
    "    exit()\n",
    "# getting the map from the camera\n",
    "map = vision.getMatrix()\n",
    "# setting the scale of the map\n",
    "thymio.set_scale(vision.getScale())\n",
    "print(\"MAIN: scale: \", vision.getScale())\n",
    "# setting the initial position and angle of the Thymio\n",
    "thymio.set_pose(pos_vision, angle_vision)\n",
    "# initializing the Kalman filter\n",
    "filter.initialize_position(thymio.cells_to_mm(pos_vision[0]), thymio.cells_to_mm(pos_vision[1]), angle_vision)\n",
    "print(\"MAIN: initialized kalman: \", filter.get_state())\n",
    "# computing the path from the initial position to the goal\n",
    "print(\"MAIN: computing path\")\n",
    "goal_pos = vision.getGoal()\n",
    "print(\"MAIN: goal: \", goal_pos)\n",
    "print(\"MAIN: start: \", pos_vision)\n",
    "globalPlanning.set_magnification(vision.getScale(), thymio.get_wheel_distance())\n",
    "path = globalPlanning.dijkstra(map.copy(), pos_vision, goal_pos)\n",
    "if path == None:\n",
    "    print(\"MAIN: ERROR: no path found\")\n",
    "    exit()\n",
    "print(\"MAIN: path: \", path)\n",
    "# setting the map and the path to the plotter\n",
    "plotter.set_map(map, pos_vision, goal_pos)\n",
    "plotter.plot_map()\n",
    "# setting the path to the Thymio object\n",
    "thymio.set_path(path)\n",
    "print(\"MAIN: reduced path\", thymio.get_path_cells())\n",
    "plotter.plot_path(thymio.get_path_cells())\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# update time\n",
    "old_time = time.time_ns()\n",
    "vision.update_image()\n",
    "\n",
    "\n",
    "while not goal:\n",
    "    print(\"MAIN: iter: \", iter)\n",
    "    print(\"MAIN: position: \", position)\n",
    "    print(\"MAIN: angle: \", angle)\n",
    "    print(\"MAIN: dt: \", dt)\n",
    "\n",
    "    vision.update_image()\n",
    "    pos_vision = vision.getStart()\n",
    "    #pos_vision = [thymio.mm_to_cells(thymio.get_position()[0]), thymio.mm_to_cells(thymio.get_position()[1])]\n",
    "    angle_vision = vision.getAngle()\n",
    "    #angle_vision = thymio.get_angle()\n",
    "\n",
    "    # check if camera is obstructed\n",
    "    if pos_vision is not None:\n",
    "        # position here is the measurement from the camera\n",
    "        print(\"MAIN: Camera not obstructed, getting position from camera\")\n",
    "        \n",
    "        # check if the robot has been kidnapped\n",
    "        # if the position from the camera is too far from the Kalman filter prediction\n",
    "        if thymio.amIKidnapped(pos_vision, angle_vision):\n",
    "            print(\"MAIN: kidnapping detected\")\n",
    "\n",
    "            # tmdclient function to stop the motors\n",
    "            motor_stop()\n",
    "\n",
    "            # update the map with new robot position\n",
    "            time.sleep(2)\n",
    "            \n",
    "            vision.update_image()\n",
    "            pos_vision = vision.getStart()\n",
    "            angle_vision = vision.getAngle()\n",
    "            # setting the scale of the map\n",
    "            # thymio.set_scale(vision.getScale())\n",
    "            filter.initialize_position(thymio.cells_to_mm(pos_vision[0]), thymio.cells_to_mm(pos_vision[1]), angle_vision)\n",
    "\n",
    "            # new path planning\n",
    "            # map = vision.getMatrix()\n",
    "            globalPlanning.set_magnification(vision.getScale(), thymio.get_wheel_distance())\n",
    "            path = globalPlanning.dijkstra(map.copy(), pos_vision, goal_pos)\n",
    "            if path == None:\n",
    "                print(\"MAIN: ERROR: no path found\")\n",
    "                exit()\n",
    "            thymio.set_path(path)\n",
    "            plotter.plot_path(thymio.get_path_cells())\n",
    "        \n",
    "        # camera measuerement that will then be used for the Kalman filter\n",
    "        print(\"MAIN: camera measurement: \", pos_vision, angle_vision)\n",
    "        measurement = np.array([thymio.cells_to_mm(pos_vision[0]), thymio.cells_to_mm(pos_vision[1]), angle_vision])\n",
    "        print(\"MAIN: kalman measurement, before update: \", measurement)\n",
    "        filter.kalman_update(measurement)\n",
    "        print(\"MAIN: kalman update: \", filter.get_state())\n",
    "    \n",
    "    else:\n",
    "        print(\"MAIN: camera obstructed\")\n",
    "\n",
    "    # get the state from the Kalman filter after the update\n",
    "    x, y, angle = filter.get_state()\n",
    "    position = [x, y]\n",
    "    print(\"MAIN: kalman position used: \", position)\n",
    "    print(\"MAIN: kalman angle used: \", angle)\n",
    "    row_trajectory.append(float(thymio.mm_to_cells(x)))\n",
    "    col_trajectory.append(float(thymio.mm_to_cells(y)))\n",
    "\n",
    "    # update the pose of the Thymio in the ThymioControl object\n",
    "    # the update will come either from:\n",
    "    #     prediction the iteration before + update from the camera this iteration\n",
    "    #     prediction the iteration before (no update from the camera)\n",
    "    thymio.update_pose(position, angle)\n",
    "\n",
    "    # check if the robot is detecting an obstacle\n",
    "    # tmclient function to get the proximity sensors\n",
    "    prox = sensor_data()\n",
    "    print(\"MAIN: proximity sensors: \", prox)\n",
    "    if (localPlanning.is_obstacle_avoidance(prox)):\n",
    "        # move with local planning until the robot is not back on the path\n",
    "        wl, wr = localPlanning.obstacle_avoidance(prox)\n",
    "        v, w = thymio.inverseDifferentialDrive(wl, wr)\n",
    "        # do not move the first iteration, need to set the dt\n",
    "        if dt == 0:\n",
    "            wl, wr = 0, 0\n",
    "    else:\n",
    "        # move with global planning\n",
    "        v, w, wl, wr, goal = thymio.move(position, angle, dt)\n",
    "\n",
    "    print(\"MAIN: speed: \", thymio.convert_speed_cells(v), w)\n",
    "    print(\"MAIN: motor commands: \", wl, wr)\n",
    "    \n",
    "    # update the Kalman filter\n",
    "    filter.kalman_prediction(wl, wr, dt)\n",
    "    print(\"MAIN: kalman prediction: \", filter.get_state())\n",
    "    px, py, pa = filter.get_state()\n",
    "    thymio.set_pred(px, py, pa)\n",
    "    # also update the pose of the Thymio with the prediction because there is no camera update\n",
    "    thymio.update_pose([px, py], pa)\n",
    "\n",
    "    print(\"MAIN: trajectory: \", row_trajectory, col_trajectory)\n",
    "    plotter.plot_trajectory(row_trajectory, col_trajectory)\n",
    "\n",
    "    # tmdclient function to move the motors\n",
    "    motor_go(int(wl), int(wr))\n",
    "    \n",
    "    # sleep for a while\n",
    "    iter += 1\n",
    "    time.sleep(timeout)\n",
    "    dt = (time.time_ns() - old_time) / 1e9\n",
    "    old_time = time.time_ns()\n",
    "    print(\"-------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"MAIN: goal reached\")\n",
    "motor_stop()\n",
    "leds_go_crazy()\n",
    "leds_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
