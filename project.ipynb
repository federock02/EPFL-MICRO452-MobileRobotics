{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPFL-MICRO452 - Mobile Robotics Project\n",
    "Project for the Mobile Robotic course, fall semester 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "**Rocca Federico** (390233) - First year MSc in Robotics at EPFL, previous degree BSc in Computer Engineering at Politecnico di Milano  \n",
    "**Rashidi Mohammad Massi** (394309) - First year MSc in CyberSecurity at EPFL previous degree BSc in Computer Science at University of Geneva       \n",
    "**Rawas Mouhamad Bilal** (345489) - First year MSc in Robotics at EPFL, previous degree BSc in Microengineering at EPFL    \n",
    "**Schär Mikaël Joël Michel** (325388) - First year MSc in Robotics at EPFL, previous degree BSc in Microengineering at EPFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of the project is to build a system able of controlling a <a href=\"https://www.thymio.org/\" target=\"_blank\">Thymio</a> robot. This system shoud integrate all the main modules that are tipically found in a simple mobile robot:\n",
    "- **Vision**\n",
    "- **Global Navigation**\n",
    "- **Local Navigation**\n",
    "- **Filtering**\n",
    "\n",
    "The robot should will be placed in an **environment** (explained later) where there are some **permanent obstacles**. Given a **camera view** of the environment, a **global plan** should be generated, starting from the Thymio's position, leading to the goal and avoiding the permanent obstacles. Some **random obstacles** may be introduced in the environment during the movement of the robot from start to goal, and it should be able to avoid them. The robot should be localized both using the camera image, that can be obstructed on purpose from time to time, and the odometry, by fusing them with the **filtering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "Here's a demo of the system working:  \n",
    "<p align=\"center\">\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"video.mov\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "The environment that the robot has to navigate has been designed by us, and it consists of a **white floor**, the traversable space, where we placed some **black cutouts**, that represent permanent obstacles, while the goal position is indicated by a **red mark** placed on the floor.  \n",
    "<p align=\"center\">\n",
    "    <img src=\"images/cropped_image.jpeg\" width=\"600\">\n",
    "</P>\n",
    "In order to detect the white Thymio robot in the white background we decided to place an <a href=\"https://april.eecs.umich.edu/software/apriltag\" target=\"_blank\">AprilTag</a> on its top. It is used both for detecting the robot's position and orientation.\n",
    "We also attached 4 AprilTags on the 4 corners of the environment in order to straighten the image that the camera records and generate precise measurements based on that image.  \n",
    "<!-- Tag du robot -->\n",
    "<p align=\"center\">\n",
    "    <span style=\"display: block; font-weight: bold; margin-bottom: 10px;\">Tag corresponding to the robot</span>\n",
    "    <img src=\"images/tag.png\" width=\"200\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Tags des 4 coins -->\n",
    "<p align=\"center\">\n",
    "    <span style=\"display: block; font-weight: bold; margin-bottom: 10px;\">Tags corresponding to the 4 corners of the image</span>\n",
    "    <img src=\"images/tagStandard41h12-100.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/tagStandard41h12-113.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/tagStandard41h12-141.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/tagStandard41h12-283.svg\" width=\"200\" style=\"margin: 10px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Implemented\n",
    "- The **vision** module generates the map of the environment, starting from a camera image.\n",
    "- The **vision** module also tracks the pose of the robot during its movement.\n",
    "- The information from the **vision** is also used to detect and solve any kidnapping situation, by relocalizing the robot in the map\n",
    "- The **global navigation** module, given the map of the environment, with a starting position and a goal, deigns the optimal plan using the Dijkstra algorithm.\n",
    "- The **Thymio control** module generates the movement commands that allow the robot to move along the path from start to finish\n",
    "- The **local navigation** module is used for avoiding the unexpected obstacles that might be detected along the path\n",
    "- The **filtering** module implements a Kalman filter in order to better localize the robot. It predicts the robots position given the movements made and updates the prediction using data from the vision.\n",
    "- The **filtering** is also used for having an estimate of the position when the camera is covered and allow the Thymio to keep moving throwards the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Implemented Modules\n",
    "The next sections will explain how each module is implemented and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision**\n",
    "\n",
    "The Vision system, implemented in the <code>Vision</code> class in <code>vision.py</code> detects the 4 corners of the map using **AprilTags** to align and straighten the image, identifies the robot’s position and orientation with a specific tag, locates the goal by detecting red zones, and generates a matrix that represents the map by highlighting obstacles, which correspond to the black areas on the map, and the accessible areas available for the Thymio robot to navigate. The system calculates and returns the map scale in pixels per millimiters for converting positions on the grid map to real world measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><b>Demo of Vision System</b></h3>\n",
    "\n",
    "<!-- Step 1: Original Image -->\n",
    "<p align=\"center\">\n",
    "    <strong>1. Original Image Captured</strong><br>\n",
    "    <img src=\"images/original_image.jpeg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 2: Cropped Image -->\n",
    "<p align=\"center\">\n",
    "    <strong>2. Cropped and Aligned Image</strong><br>\n",
    "    <img src=\"images/cropped_image.jpeg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 3: Image with Start and Goal Highlighted -->\n",
    "<p align=\"center\">\n",
    "    <strong>3. Image with the center of the Start and the Goal Highlighted</strong><br>\n",
    "    <img src=\"images/start_goal_highlighted.jpg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 4: Image with Red Regions and Tags Whitened -->\n",
    "<p align=\"center\">\n",
    "    <strong>4. Image with Red Regions and Robot's Tags Whitened</strong><br>\n",
    "    <img src=\"images/whitened_regions.jpg\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "<!-- Step 5: Final Map with Obstacles Highlighted -->\n",
    "<p align=\"center\">\n",
    "    <strong>5. Final Binary Map Showing Obstacles</strong><br>\n",
    "    <img src=\"images/binary_obstacle_map.jpg\" width=\"500\" style=\"margin: 7px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Planning**\n",
    "The goal of this project is guide the robot from a start position to a goal position. To make this happen, it is necessary to implement a **global navigation** module, implemented in the <code>GlobalPlanning</code> class in <code>globalPlanning.py</code>. It works by building a path that our Thymio will follow, from the position of the robot to the position of our goal placed on our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dijkstra's Algorithm**\n",
    "\n",
    "This algorithm is implemented on a grid representation of the map. In this grid it will search the most efficient path to reach our goal. Here an exemple of a grid taken from exercice session 5 :\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Grid with the start \"S\" and the goal \"G\"</strong><br>\n",
    "    <img src=\"images/Dijkstra 1.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "-The first step is to assign a cost to all the cell of the map. To do this at each step from the start, we take all the neighbors not already evaluated and assign each one of them the cost of the current cell incremented by one. An exemple is provide for better understanding:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Grid with the first iterations of the algorithm</strong><br>\n",
    "    <img src=\"images/Dijkstra 2.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "-The second step is to find the less costly path. To find it, we take our grid and start from the goal. We take all the neighbors and select the cell with the smallest number as part of the path. Then we select the last cell added to the path, find the neighbor with lowest cost and add it to the path, repeating these steps untill we reach the start position. After that we can return the path by reversing the sequence of cells we built. Here an exemple of the shortest path between start and goal:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>Grid with the optimal path in gray</strong><br>\n",
    "    <img src=\"images/Dijkstra 3.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n",
    "In our code the algorithm is implemented with the 8-connected grid, so we also consider as neighbors the cells on the diagonal direction. We can see the difference between 4-connectivity and 8-connectivity in the example here below:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <strong>4 and 8-connected grid</strong><br>\n",
    "    <img src=\"images/4-connected grid.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "    <img src=\"images/8-connected grid.png\" width=\"250\" style=\"margin: 10px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Planning**\n",
    "Once the global navigation module has designed a path from the start position to the goal, the local planning module has to generate the movement commands for the robot in such a way that it will follow that path while avoiding unexpected obstacles that might be detected while moving from the start position to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motion Control**\n",
    "The motion control, that is the process of generating motion commands to get the robot to follow the path generated by the global plannign module, is implemented in the <code>ThymioControl</code> class in <code>thymioControl.py</code>.\n",
    "\n",
    "#### **Path Reduction**\n",
    "As a first step, it reduces the path to the minumum sequence of waypoints. This means that all the points in the path that lay on the same straight line, excluding the first and last, are removed. Here is shown an example of this behavior:  \n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"images/ogPath.png\" alt=\"Original path\" style=\"width: 45%;\"/>\n",
    "  <img src=\"images/reducedPath.png\" alt=\"Reduced path\" style=\"width: 45%;\"/>\n",
    "</div>\n",
    "\n",
    "#### **Kidnapping Detection**\n",
    "This module is also used for recognizing kidnapping situations. This means that when a new measurement of position and angle is received from the camera, it is compared with the prediction of position and angle produced by the Kalman filter at the previous step, and if the position or the angle differ by more than the relative thresholds <code>self.__kidnappingThresholdPosition</code> and <code>self.__kidnappingThresholdAngle</code>, a kidnapping situation is signaled and the kidnapping routine, explained later, is run.\n",
    "\n",
    "#### **Movement Generation**\n",
    "Given the current position and angle of the Thymio and the position of the next waypoint in the path that needs to be reached, the required velocity commands are calculated in this module. Two different approaches are present in the file: <code>move_pd(position, angle)</code> or <code>move(position, angle)</code>\n",
    "- <code>move_pd()</code>: the linear speed is considered always constant, while the angular speed is calculated using a PD controller, based on the angle difference between the Thymio orientation and the relative angle of the waypoint in respect to the robot for the proportional part and on the previous angle difference for the derivative part. The linear and angular speed are then used to compute the left and right motor speeds using differential drive kinematics.  \n",
    "  PD controller: $$w = k_p \\cdot \\text{angleDistance} + k_d \\cdot \\frac{\\text{angleDistance} - \\text{previousAngleDistance}}{dt}$$, with $k_p$ and $k_d$ respectively the proportional and derivative gain\n",
    "- <code>move()</code>:this time, when an angle difference is detected between the Thymio's orientation and the direction throwards the waypoint, the robot is stopped (linear speed = 0), and it is turned on the spot. The angular velocity used for turning is once again calculated using a PD controller. If instead the angle difference is less then the threshold <code>self.__angleThreshold</code>, the angular velocity is set to 0 and the linear velocity is set to a predetermined constant value, making the Thymio go straight throwards the waypoint.\n",
    "\n",
    "Both methods check if the current waypoint has been reached, by computing the distance from the robot to the waypoint and checking if it is less than the threshold <code>self.__reachedThreshold</code>, and if the waypoint has been overshot, maybe during a phase of obstacle avoidance, by comparing the distances between the robot and the next waypoint and betweent the current waypoint and the next wayopint; in both cases it would move onto the next waypoint.\n",
    "\n",
    "While the first method never stops the robot, so it is potentially faster and smoother, tuning the parameters $k_p$ and $k_d$ wrongly may lead to overshoots or oscillations. On the other hand, the stop-rotate-go controller is more precise but obviously slower, since the robot has to stop before turning on the spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Local Avoidance**\n",
    "An important part in robot's navigation is the avoidance of unexpected obstacles that might be detected while moving from the start position to the goal, following the global plan. The process of local avoidance consists of detecting the obstacles using the robots sensors and designing a more or less efficient plan to get around it, avoiding collisions, and get back on the predetermined global plan.\n",
    "The python file <code>localPlanning.py</code> contains the <code>LocalPlanning</code> class that implements the local avoidance modules.\n",
    "\n",
    "#### **Obstacle Detection**\n",
    "When the <code>self.is_obstacle_avoidance(prox_horizontal)</code> method is called, a boolean value is returned, <code>True</code> if any of the sensors is reading higher than the threshold, <code>False</code> otherwise.\n",
    "\n",
    "#### **Obstacle Avoidance**\n",
    "If an obstacle is detected, the obstacle avoidance routine <code>self.obstacle_avoidance(prox_horizontal)</code> generates directly the motor commands **wl** and **wl** by multiplying the values recorded by the proximity sensors by two simmetric arrays of weights, one for each motor, virtually generating a push, away from the obstacle. This implements a really simple neural network, with one neuron for each motor, that takes the sensor values as inputs and computes the motor command. This approach is inspired by what was proposed in exercise session 3.\n",
    "<p align=\"center\">\n",
    "    <strong>Neural Network used for Obstacle Avoidance</strong><br>\n",
    "    <img src=\"images/ANN_robot_control.png\" width=\"500\" style=\"margin: 10px;\">\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kalman Filter**\n",
    "\n",
    "The Kalman Filter is a mathematical tool used in this project to estimate the robot's position and orientation (state) while navigating toward a goal. The robot uses data from two sources: **odometry** (wheel speeds) for prediction and a **camera** for occasional updates. Since both these sources are noisy and prone to errors, the Kalman Filter combines their information to produce an accurate and smooth estimate of the robot’s state. It is implemented in the <code>Kalman</code> class in <code>kalman.py</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Nonlinear Motion Model**\n",
    "\n",
    "A differential-drive robot's motion is inherently nonlinear because its position $(x, y)$ and orientation $\\theta$ depend on trigonometric relationships. For example, its state evolves as (motion model):\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t + v \\cos(\\theta) \\Delta t, \\quad y_{t+1} = y_t + v \\sin(\\theta) \\Delta t, \\quad \\theta_{t+1} = \\theta_t + \\omega \\Delta t\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $v$: Linear velocity (derived from wheel speeds).\n",
    "- $\\omega$: Angular velocity (based on differential wheel motion).\n",
    "- $\\Delta t$: Time step.\n",
    "\n",
    "The EKF linearizes these equations using the Jacobian of the motion model. In the **Kalman class**, the motion model is embedded in the matrix $\\mathbf{G}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fusing Odometry and Camera Data**\n",
    "\n",
    "Odometry (wheel encoder data) is prone to errors such as:\n",
    "\n",
    "- **Drift** over time, accumulating inaccuracies as the robot moves.\n",
    "- **Wheel slippage** and uneven terrain causing deviations from the true trajectory.\n",
    "\n",
    "The camera, while more accurate, provides intermittent data due to potential obstructions or missed detections. The EKF combines these two sources of data to provide a reliable state estimate:\n",
    "\n",
    "1. **Prediction Step**: This step uses the odometry data to predict the robot's next state. It always runs, regardless of whether camera data is available, ensuring continuity in state estimation. The prediction compensates for missing measurements and accounts for process noise.\n",
    "\n",
    "2. **Update Step**: This step completes the prediction using camera measurements when available. It refines the estimated state by fusing the relatively accurate camera data with the prediction. The update step only runs if the camera is not obstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How the Kalman Filter Works**\n",
    "\n",
    "**A. Prediction Step**\n",
    "\n",
    "Using the robot's last known state, wheel speeds, and time elapsed (Δt), the Kalman Filter predicts the robot's next state:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{pred}} = \\mathbf{A} \\cdot \\mathbf{E} + \\mathbf{G} \\cdot \\mathbf{U}\n",
    "$$\n",
    "\n",
    "- **A**: State transition matrix (accounts for constant movement without changes).\n",
    "- **B**: Control matrix (maps wheel speeds to motion).\n",
    "- **U**: Control vector (wheel speeds).\n",
    "\n",
    "It also predicts the **uncertainty** in the state, denoted by the covariance matrix **P**:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{\\text{pred}} = \\mathbf{A} \\cdot \\mathbf{P} \\cdot \\mathbf{A}^T + \\mathbf{R}\n",
    "$$\n",
    "\n",
    "- **Q**: Process noise covariance (uncertainty from wheel encoders).\n",
    "\n",
    "**B. Update Step**\n",
    "\n",
    "When the camera provides a measurement **Z**, the Kalman Filter updates the state and uncertainty:\n",
    "\n",
    "1. Compute the Kalman gain **K**, which determines how much weight to give to the measurement:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\mathbf{P}_{\\text{pred}} \\cdot \\mathbf{H}^T \\cdot \\left(\\mathbf{H} \\cdot \\mathbf{P}_{\\text{pred}} \\cdot \\mathbf{H}^T + \\mathbf{Q}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "- **H**: Measurement matrix (maps state to camera readings).\n",
    "- **R**: Measurement noise covariance (uncertainty in camera data).\n",
    "\n",
    "2. Update the state using the measurement:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{update}} = \\mathbf{E}_{\\text{pred}} + \\mathbf{K} \\cdot \\left(\\mathbf{Z} - \\mathbf{H} \\cdot \\mathbf{E}_{\\text{pred}}\\right)\n",
    "$$\n",
    "\n",
    "3. Update the uncertainty:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{\\text{update}} = (\\mathbf{I} - \\mathbf{K} \\cdot \\mathbf{H}) \\cdot \\mathbf{P}_{\\text{pred}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Behavior in the Control Loop**\n",
    "\n",
    "1. **Prediction Always Happens**:\n",
    "    * The prediction step runs at every iteration using odometry.\n",
    "    * This ensures the robot always has an estimated position, even if the camera data is unavailable.\n",
    "\n",
    "2. **Update Happens When the Camera Sees the Robot**:\n",
    "    * If the camera detects the robot, the Kalman Filter corrects its prediction using the more accurate camera measurement.\n",
    "\n",
    "3. **Handling Obstructions**:\n",
    "    * If the camera is obstructed, the filter relies solely on the prediction step until the camera resumes detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of Parameters\n",
    "The following section explains how the various parameters used throughout the project where selected and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Obstacle Avoidance**\n",
    "- **Obstacle Detection Threshold**: the Thymio robot features 5 horizontal **proximity sensors** it its front part (see Thyimio cheat sheet snippet) that can be used to detect obstacles using infrared technology. The range of values that the sensors return is [0, ~4300] and the updates come at a frequency of 10Hz.\n",
    "    <p align=\"center\">\n",
    "        <img src=\"images/thymio_cheat_sheet1.png\" width=\"800\">\n",
    "    </p>\n",
    "    It is possible to map the readings from the proximity sensors to the real world distance measurements in order to tune the threshold for activating the local avoidance routine. In order to do so, many measurements were taken with an obstacle at a known distance and the average of the values was computed:  \n",
    "\n",
    "    | Distance | Sensor Reading (average value) |\n",
    "    | --- | ---: |\n",
    "    | 5 | 4300 |\n",
    "    | 10 | 3095 |\n",
    "    | 15 | 2350 |\n",
    "    | 20 | 1690 |\n",
    "\n",
    "    We chose the vector <code>[3200, 3000, 2900, 3000, 3200]</code> for the thresholds relative to each front sensor, meaning a distance of a little bit more of 10 cm. Detection close to the ceter has a lower threshold with respect to an obstacle on the side because it is necessary to start avoiding before if the obstacle is located straight in frony of the robot.\n",
    "\n",
    "- **Weights for the ANN in Obstacle Avoidance**: as said before, obstacle avoidance is implemented by passing the proximity sensors' readings through a simple NN. The vector of values is multiplied by a vector of weights, one for each motor, in order to determine the wheel commands necessary for avoiding the obstacle. The weights have been chosen by trial and error, in order to generate a strong enough push away from the obstacle without disrupting too much the trajectory.\n",
    "\n",
    "#### **Thymio Control**\n",
    "- **Conversion ratio from Thymio wheel commands (PWM) to mm/s**:  \n",
    "   This was calculated by measuring the distance covered over time using ground sensor peaks, by driving the Thymio over evenly spaced stripes, as done during execise session 4. The Thymio's speed in mm/s was divided by the corresponding PWM value to determine the conversion ratio.  \n",
    "\n",
    "- **Motor speed adjustment scalar**:  \n",
    "   A scalar adjustment was applied to one motor’s PWM to correct for a speed imbalance between the left and right motors. This value was determined empirically through testing to ensure straight-line movement.  \n",
    "\n",
    "#### **Kalman Filter**\n",
    "- **Variance of speed in mm²/s²**:  \n",
    "   The speed variance was computed as the variance of the recorded speed data (in mm/s) after converting using the conversion ratio and excluding initial transients. The variance was split equally between the **speed state** variance and the **speed measurement** variance, each being half of the total variance. This is again inspired by the approach in exercise session 4.  \n",
    "\n",
    "#### **Global Planning**\n",
    "- **Magnification for obstacles**:\n",
    "   When computing the global plan the obstacles need to be enlarged so that the path does not pass too close to the obstacles. This would be a problem because the position of the Thymio is determined by the center point of the tag on its top, so if the robot was to follow a path really close to an obstacle, its wheel would be inside the obstacle.\n",
    "   The magnification parameter is calculated as follows: $\\text{magnification} = (\\text{thymioWheelDistance} * 1.5) / 2$, by increasing the robot's size by a factor of 150% and then dividing by 2 since the position is considered along the center line of the robot. It is then converted in number of cells.\n",
    "\n",
    "#### **Vision**\n",
    "- **Color Threshold**\n",
    "   We hand-tuned the threshold used for discriminating obstacles, so black pixels, from backgroung, so white/grayish pixels, in the image recorded by the camera. This threshold might need to be adjusted based on light conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "The next section presents the runnable cells that allow the system to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First step, all the classes that have been explained above need to be imported.\n",
    "In order to be sure that all the python packages are correctly installed and ready to use, it is suggested to run <code>pip install -r requirements.txt</code> to get all of them. It should be noted that the <code>pupil-apriltags</code> package may give some problems with python versions higer than 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the classes in the package\n",
    "from vision import Vision\n",
    "from globalPlanning import GlobalPlanning\n",
    "from thymioControl import ThymioControl\n",
    "from localPlanning import LocalPlanning\n",
    "from kalman import Kalman\n",
    "from plotter import Plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install the <code>tdmclient</code> package, used for connecting and comunicating with the Thymio. The control from the notebook is made possible by the import of <code>tdmclient.notebook</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tdmclient\n",
    "!pip3 install tdmclient --upgrade\n",
    "\n",
    "# import the TDMClient module\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TdmClient Functions\n",
    "The following functions are used for modifying the variables in the Thymio, by comunicating through the tdmclient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move with motor command left and right\n",
    "@tdmclient.notebook.sync_var\n",
    "def motor_go(left,right):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = left\n",
    "    motor_right_target = right\n",
    "\n",
    "# stop the motors\n",
    "@tdmclient.notebook.sync_var\n",
    "def motor_stop():\n",
    "    global motor_left_target,motor_right_target\n",
    "    motor_left_target = 0\n",
    "    motor_right_target = 0\n",
    "    \n",
    "# get the readings from the proximity sensors\n",
    "@tdmclient.notebook.sync_var\n",
    "def sensor_data():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal.copy()\n",
    "\n",
    "# turn off the leds\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_off():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 0, 0]\n",
    "    leds_bottom_left = [0, 0, 0]\n",
    "    leds_bottom_right = [0, 0, 0]\n",
    "    leds_circle = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# celebrate with the leds\n",
    "def leds_go_crazy():\n",
    "    for i in range(8):\n",
    "        leds_1()\n",
    "        time.sleep(0.1)\n",
    "        leds_2()\n",
    "        time.sleep(0.1)\n",
    "        leds_3()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# set the leds to a specific color\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_1():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [32, 0, 0]\n",
    "    leds_bottom_left = [0, 32, 0]\n",
    "    leds_bottom_right = [0, 0, 32]\n",
    "    leds_circle = [32, 32, 32, 32, 0, 0, 0, 0]\n",
    "\n",
    "# set the leds to a specific color\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_2():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 32, 0]\n",
    "    leds_bottom_left = [0, 0, 32]\n",
    "    leds_bottom_right = [32, 0, 0]\n",
    "    leds_circle = [0, 0, 0, 0, 32, 32, 32, 32]\n",
    "\n",
    "# set the leds to a specific color\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_3():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 0, 32]\n",
    "    leds_bottom_left = [32, 0, 0]\n",
    "    leds_bottom_right = [0, 32, 0]\n",
    "    leds_circle = [32, 0, 32, 0, 32, 0, 32, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Loop\n",
    "The next code section presents the main control loop that unites all the modules and functionalities. It is implemented following the finite state machine here below:\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/thymioStateDiagram.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the FSM above, the first steps that need to be taken are the initalizations of the various classes, like getting the first image from the vision, localizing the Thymio, generating the global plan and initializing the first position in the Kalman filter.\n",
    "After that, the main loop can start. The loop runs untill the goal is reached and it consists in the next steps:\n",
    "1. getting the observation from the vision\n",
    "2. if the observation is not null, updating the Kalman position (*)\n",
    "3. if the sensors are detecting an obstacle, run the obstacle avoidance routine\n",
    "4. otherwhise, if no obstacle is detected, based on the position from the Kalman and the next waypoint, generate the movement commands\n",
    "5. predict the future position based on the motor commands, using the Kalman prediction\n",
    "\n",
    "(*) if the camera returns a non-null position, it is also required to check for kidnapping before the Kalman update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISION: tag IDs: []\n",
      "VISION: warning: Only 0 tags detected. Cannot crop the image.\n",
      "MAIN: initializing vision\n",
      "MAIN: Getting map and Thymio position from camera\n",
      "MAIN: start:  None\n",
      "MAIN: angle:  None\n",
      "MAIN: ERROR: camera obstructed in the first iteration\n",
      "MAIN: scale:  1\n",
      "THYMIO: pos:  []\n",
      "THYMIO: angle:  None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m thymio\u001b[38;5;241m.\u001b[39mset_pose(pos_vision, angle_vision)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# initializing the Kalman filter\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_position(thymio\u001b[38;5;241m.\u001b[39mcells_to_mm(\u001b[43mpos_vision\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m), thymio\u001b[38;5;241m.\u001b[39mcells_to_mm(pos_vision[\u001b[38;5;241m1\u001b[39m]), angle_vision)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAIN: initialized kalman: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m.\u001b[39mget_state())\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# computing the path from the initial position to the goal\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# leds off during the navigation\n",
    "leds_off()\n",
    "\n",
    "# initialize objects\n",
    "globalPlanning = GlobalPlanning()\n",
    "localPlanning = LocalPlanning()\n",
    "filter = Kalman()\n",
    "thymio = ThymioControl()\n",
    "plotter = Plotter()\n",
    "\n",
    "# initialize variables for saving the trajectory and the Kalman filter positions\n",
    "row_trajectory = []\n",
    "col_trajectory = []\n",
    "row_kalman_pred = []\n",
    "col_kalman_pred = []\n",
    "\n",
    "# timeout for the loop\n",
    "timeout = 0.1\n",
    "# delta time between iterations, considering all the computation time\n",
    "dt = 0\n",
    "# old time to compute the delta time\n",
    "old_time = time.time_ns()\n",
    "\n",
    "# create a vision object\n",
    "image_path = \"images/original_image.jpeg\"\n",
    "vision = Vision(fps=3, threshold=60, target_height=80, default_image_path=image_path)\n",
    "\n",
    "# iteration counter\n",
    "iter = 1\n",
    "\n",
    "# variables for position and angle\n",
    "position = None\n",
    "angle = None\n",
    "\n",
    "# flag to check if the goal is reached\n",
    "goal = False\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# initialization from vision\n",
    "vision.update_image()\n",
    "# getting the initial position and angle from the camera\n",
    "pos_vision = vision.getStart()\n",
    "angle_vision = vision.getAngle()\n",
    "print(\"MAIN: start: \", pos_vision)\n",
    "print(\"MAIN: angle: \", angle_vision)\n",
    "if pos_vision is None:\n",
    "    print(\"MAIN: ERROR: camera obstructed in the first iteration\")\n",
    "    exit()\n",
    "# getting the map from the camera\n",
    "map = vision.getMatrix()\n",
    "# setting the scale of the map\n",
    "thymio.set_scale(vision.getScale())\n",
    "# setting the initial position and angle of the Thymio\n",
    "thymio.set_pose(pos_vision, angle_vision)\n",
    "# initializing the Kalman filter\n",
    "filter.initialize_position(thymio.cells_to_mm(pos_vision[0]), thymio.cells_to_mm(pos_vision[1]), angle_vision)\n",
    "# computing the path from the initial position to the goal\n",
    "goal_pos = vision.getGoal()\n",
    "globalPlanning.set_magnification(vision.getScale(), thymio.get_wheel_distance())\n",
    "path = globalPlanning.dijkstra(map.copy(), pos_vision, goal_pos)\n",
    "if path == None:\n",
    "    print(\"MAIN: ERROR: no path found\")\n",
    "    exit()\n",
    "# setting the map and the path to the plotter\n",
    "plotter.set_map(map, pos_vision, goal_pos)\n",
    "# setting the path to the Thymio object\n",
    "thymio.set_path(path)\n",
    "plotter.update_path(thymio.get_path_cells())\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# update time\n",
    "old_time = time.time_ns()\n",
    "vision.update_image()\n",
    "\n",
    "\n",
    "while not goal:\n",
    "    vision.update_image()\n",
    "    pos_vision = vision.getStart()\n",
    "    #pos_vision = [thymio.mm_to_cells(thymio.get_position()[0]), thymio.mm_to_cells(thymio.get_position()[1])]\n",
    "    angle_vision = vision.getAngle()\n",
    "    #angle_vision = thymio.get_angle()\n",
    "\n",
    "    # check if camera is obstructed\n",
    "    if pos_vision is not None:\n",
    "        # position here is the measurement from the camera\n",
    "        \n",
    "        # check if the robot has been kidnapped\n",
    "        # if the position from the camera is too far from the Kalman filter prediction\n",
    "        if thymio.amIKidnapped(pos_vision, angle_vision):\n",
    "\n",
    "            # tmdclient function to stop the motors\n",
    "            motor_stop()\n",
    "\n",
    "            # update the map with new robot position\n",
    "            time.sleep(2)\n",
    "            \n",
    "            vision.update_image(live=False)\n",
    "            pos_vision = vision.getStart()\n",
    "            angle_vision = vision.getAngle()\n",
    "            # setting the scale of the map\n",
    "            thymio.set_scale(vision.getScale())\n",
    "            filter.initialize_position(thymio.cells_to_mm(pos_vision[0]), thymio.cells_to_mm(pos_vision[1]), angle_vision)\n",
    "\n",
    "            # new path planning\n",
    "            map = vision.getMatrix()\n",
    "            globalPlanning.set_magnification(vision.getScale(), thymio.get_wheel_distance())\n",
    "            path = globalPlanning.dijkstra(map.copy(), pos_vision, goal_pos)\n",
    "            if path == None:\n",
    "                print(\"MAIN: ERROR: no path found\")\n",
    "                exit()\n",
    "            thymio.set_path(path)\n",
    "            plotter.update_path(thymio.get_path_cells())\n",
    "\n",
    "        # camera measuerement that will then be used for the Kalman filter\n",
    "        measurement = np.array([thymio.cells_to_mm(pos_vision[0]), thymio.cells_to_mm(pos_vision[1]), angle_vision])\n",
    "        filter.kalman_update(measurement)\n",
    "\n",
    "    # get the state from the Kalman filter after the update\n",
    "    x, y, angle = filter.get_state()\n",
    "    position = [x, y]\n",
    "    plotter.update_traj(float(thymio.mm_to_cells(x)), float(thymio.mm_to_cells(y)))\n",
    "\n",
    "    # update the pose of the Thymio in the ThymioControl object\n",
    "    # the update will come either from:\n",
    "    #     prediction the iteration before + update from the camera this iteration\n",
    "    #     prediction the iteration before (no update from the camera)\n",
    "    thymio.update_pose(position, angle)\n",
    "\n",
    "    # check if the robot is detecting an obstacle\n",
    "    # tmclient function to get the proximity sensors\n",
    "    prox = sensor_data()\n",
    "    if (localPlanning.is_obstacle_avoidance(prox)):\n",
    "        # move with local planning until the robot is not back on the path\n",
    "        wl, wr = localPlanning.obstacle_avoidance(prox)\n",
    "        v, w = thymio.inverseDifferentialDrive(wl, wr)\n",
    "        # do not move the first iteration, need to set the dt\n",
    "        if dt == 0:\n",
    "            wl, wr = 0, 0\n",
    "    else:\n",
    "        # move with global planning\n",
    "        v, w, wl, wr, goal = thymio.move(position, angle, timeout)\n",
    "    \n",
    "    # update the Kalman filter\n",
    "    filter.kalman_prediction(wl, wr, dt)\n",
    "    px, py, pa = filter.get_state()\n",
    "    thymio.set_pred(px, py, pa)\n",
    "    # also update the pose of the Thymio with the prediction because there is no camera update\n",
    "    thymio.update_pose([px, py], pa)\n",
    "\n",
    "    plotter.update_pred(thymio.mm_to_cells(px), thymio.mm_to_cells(py))\n",
    "\n",
    "    # tmdclient function to move the motors\n",
    "    motor_go(int(wl), int(wr))\n",
    "    \n",
    "    # sleep for a while\n",
    "    iter += 1\n",
    "    time.sleep(timeout)\n",
    "    dt = (time.time_ns() - old_time) / 1e9\n",
    "    old_time = time.time_ns()\n",
    "\n",
    "motor_stop()\n",
    "leds_go_crazy()\n",
    "leds_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
