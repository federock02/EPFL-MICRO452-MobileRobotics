{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPFL-MICRO452 - Mobile Robotics Project\n",
    "Project for the Mobile Robotic course, fall semester 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "**Rocca Federico** (390233) - First year MSc in Robotics at EPFL, previous degree BSc in Computer Engineering at Politecnico di Milano  \n",
    "**Rashidi Mohammad Massi** () -  \n",
    "**Rawas Mouhamad Bilal** () -  \n",
    "**Schär Mikaël Joël Michel** (325388) - First year MSc in Robotics at EPFL, previous degree BSc in Microengineering at EPFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of the project is to build a system able of controlling a <a href=\"https://www.thymio.org/\" target=\"_blank\">Thymio</a> robot. This system shoud integrate all the main modules that are tipically found in a simple mobile robot:\n",
    "- **Vision**\n",
    "- **Global Navigation**\n",
    "- **Local Navigation**\n",
    "- **Filtering**\n",
    "\n",
    "The robot should will be placed in an **environment** (explained later) where there are some **permanent obstacles**. Given a **camera view** of the environment, a **global plan** should be generated, starting from the Thymio's position, leading to the goal and avoiding the permanent obstacles. Some **random obstacles** may be introduced in the environment during the movement of the robot from start to goal, and it should be able to avoid them. The robot should be localized both using the camera image, that can be obstructed on purpose from time to time, and the odometry, by fusing them with the **filtering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "Here's a demo of the system working:  \n",
    "<p align=\"center\">\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"video.mov\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "The environment that the robot has to navigate has been designed by us, and it consists of a **white floor**, the traversable space, where we placed some **black cutouts**, that represent permanent obstacles, while the goal position is indicated by a **red mark** placed on the floor.  \n",
    "<p align=\"center\">\n",
    "    <img src=\"images/env_image.png\" width=\"400\">\n",
    "</P>\n",
    "In order to detect the white Thymio robot in the white background we decided to place an <a href=\"https://april.eecs.umich.edu/software/apriltag\" target=\"_blank\">AprilTag</a> on its top. It is used both for detecting the robot's position and orientation.\n",
    "We also attached 4 AprilTags on the 4 corners of the environment in order to straighten the image that the camera records and generate precise measurements based on that image.  \n",
    "<p align=\"center\">\n",
    "    <img src=\"images/tag.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Implemented\n",
    "- The **vision** module generates the map of the environment, starting from a camera image.\n",
    "- The **vision** module also tracks the pose of the robot.\n",
    "- The information from the **vision** is also used to detect and solve any kidnapping situation, by relocalizing the robot in the map\n",
    "- The **global navigation** module, given the map of the environment, with a starting position and a goal, deigns the optimal plan using the Dijkstra algorithm.\n",
    "- The Thymio control module generates the movement commands that allow the robot to move along the path from start to finish\n",
    "- The **local navigation** module is used for avoiding the unexpected obstacles that might be detected along the path\n",
    "- The **filtering** module implements a Kalman filter in order to better localize the robot. It predicts the robots position given the movements made and updates the prediction using data from the vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Implemented Modules\n",
    "The next sections will explain how each module is implemented and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Planning**\n",
    "The goal of this project is to achieve a position with the robot. To make this happen, we will use the global navigation. It's the same principle than follow a GPS, we build a path that our thymio will follow. This path start from the position of the thymio, and ends at the position of our goal placed on our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Planning**\n",
    "Once the global navigation module has designed a path from the start position to the goal, the local planning module has to generate the movement commands for the robot in such a way that it will follow that path, all while avoiding unexpected obstacles that might be detected while moving from the start position to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motion Control**\n",
    "The motion control, that is the process of generating motion commands to get the robot to follow the path generated by the global plannign module, is implemented in the <code>ThymioControl</code> class in <code>thymioControl.py</code>\n",
    "\n",
    "#### **Path Reduction**\n",
    "As a first step, it reduces the path to the minumum sequence of waypoints. This means that all the points in the path that lay on the same straight line, excluding the first and last, are removed. Here is shown an example of this behavior.  \n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"images/ogPath.png\" alt=\"Original path\" style=\"width: 45%;\"/>\n",
    "  <img src=\"images/reducedPath.png\" alt=\"Reduced path\" style=\"width: 45%;\"/>\n",
    "</div>\n",
    "\n",
    "#### **Kidnapping Detection**\n",
    "This module is also used for recognizing kidnapping situations. This means that when the pose of the Thymio is updated, based on the measurement from the camera, it is compared with the previous pose, and if the position or the angle differ by more than the relative thresholds <code>self.__kidnappingThresholdPosition</code> and <code>self.__kidnappingThresholdAngle</code>, a kidnapping situation is signaled and the kidnapping routine, explained later, is run.\n",
    "\n",
    "#### **Movement Generation**\n",
    "Given the current position and angle of the Thymio and the position of the next waypoint in the path that needs to be reached, the required velocity commands are calculated in this module. Two different approaches are present in the file: <code>move_pd(position, angle)</code> or <code>move(position, angle)</code>\n",
    "- <code>move_pd</code>: computes the distance from the robot to the waypoint, and if it is less than the threshold <code>self.__reachedThreshold</code> it considers the waypoint reached and moves on to the next. The linear speed is considered always constant, while the angular speed is calculated using a PD controller, based on the angle difference between the Thymio orientation and the relative angle of the waypoint in respect to the robot and on the previous angle difference. The linear and angular speed are then used to compute the left and right motor speeds using differential drive kinematics.  \n",
    "  PD controller: $w = k_p \\cdot \\text{angleDistance} + k_d \\cdot \\frac{\\text{angleDistance} - \\text{previousAngleDistance}}{dt}$, with $k_p$ and $k_d$ respectively the proportional and derivative gain\n",
    "- <code>move</code>: computes the distance from the robot to the waypoint, and if it is less than the threshold <code>self.__reachedThreshold</code> it considers the waypoint reached and moves on to the next. This time, when an angle difference is detected between the Thymio's orientation and the direction throwards the waypoint, the robot is stopped (linear speed = 0), and it is turned on the spot. The angolar velocity used for turned is once again calculated using a PD controller. If instead the angle difference is less then the threshold <code>self.__angleThreshold</code>, the angolar velocity is set to 0 and the linear velocity is set to a predetermined constant value, making the Thymio go straight throwards the waypoint.\n",
    "While the first method never stops the robot, so it is potentially faster and smoother, tuning the parameters $k_p$ and $k_d$ wrongly may lead to overshoots or oscillations. On the other hand, the stop-rotate-go controller is more precise but obviously slower, since the robot has to stop before turning on the spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Avoidance\n",
    "An important part in robot's navigation is the avoidance of unexpected obstacles that might be detected while moving from the start position to the goal, following the global plan. The process of local avoidance consists of detecting the obstacles using the robots sensors and designing a more or less efficient plan to get around it, avoiding collisions, and get back on the predetermined global plan.\n",
    "The python file <code>localPlanning.py</code> contains the <code>LocalPlanning</code> class that implements the local avoidance modules.\n",
    "\n",
    "#### Obstacle Detection\n",
    "The Thymio robot features 5 horizontal proximity sensors it its front part (see Thyimio cheat sheet snippet) that can be used to detect obstacles using infrared technology. The range of values that the sensors return is [0, 4300] and the updates come at a frequency of 10Hz.\n",
    "When the <code>self.is_obstacle_avoidance(prox_horizontal)</code> method is called, a boolean value is returned, <code>True</code> if any of the sensors is reading higher than the threshold, <code>False</code> otherwise.\n",
    "\n",
    "#### Obstacle Avoidance\n",
    "If an obstacle is detected, the obstacle avoidance routine <code>self.obstacle_avoidance</code> uses a \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kalman Filter**\n",
    "\n",
    "The Kalman Filter is a mathematical tool used in this project to estimate the robot's position and orientation (state) while navigating toward a goal. The robot uses data from two sources: **odometry** (wheel speeds) for prediction and a **camera** for occasional updates. Since both these sources are noisy and prone to errors, the Kalman Filter combines their information to produce an accurate and smooth estimate of the robot’s state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Nonlinear Motion Model**\n",
    "\n",
    "A differential-drive robot's motion is inherently nonlinear because its position $(x, y)$ and orientation $\\theta$ depend on trigonometric relationships. For example, its state evolves as (motion model):\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t + v \\cos(\\theta) \\Delta t, \\quad y_{t+1} = y_t + v \\sin(\\theta) \\Delta t, \\quad \\theta_{t+1} = \\theta_t + \\omega \\Delta t\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $v$: Linear velocity (derived from wheel speeds).\n",
    "- $\\omega$: Angular velocity (based on differential wheel motion).\n",
    "- $\\Delta t$: Time step.\n",
    "\n",
    "The EKF linearizes these equations using the Jacobian of the motion model. In the **Kalman class**, the motion model is embedded in the matrix $\\mathbf{G}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fusing Odometry and Camera Data**\n",
    "\n",
    "Odometry (wheel encoder data) is prone to errors such as:\n",
    "\n",
    "- **Drift** over time, accumulating inaccuracies as the robot moves.\n",
    "- **Wheel slippage** and uneven terrain causing deviations from the true trajectory.\n",
    "\n",
    "The camera, while more accurate, provides intermittent data due to potential obstructions or missed detections. The EKF combines these two sources of data to provide a reliable state estimate:\n",
    "\n",
    "1. **Prediction Step**: This step uses the odometry data to predict the robot's next state. It always runs, regardless of whether camera data is available, ensuring continuity in state estimation. The prediction compensates for missing measurements and accounts for process noise.\n",
    "\n",
    "2. **Update Step**: This step completes the prediction using camera measurements when available. It refines the estimated state by fusing the relatively accurate camera data with the prediction. The update step only runs if the camera is not obstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How the Kalman Filter Works**\n",
    "\n",
    "**A. Prediction Step**\n",
    "\n",
    "Using the robot's last known state, wheel speeds, and time elapsed (Δt), the Kalman Filter predicts the robot's next state:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{pred}} = \\mathbf{A} \\cdot \\mathbf{E} + \\mathbf{G} \\cdot \\mathbf{U}\n",
    "$$\n",
    "\n",
    "- **A**: State transition matrix (accounts for constant movement without changes).\n",
    "- **B**: Control matrix (maps wheel speeds to motion).\n",
    "- **U**: Control vector (wheel speeds).\n",
    "\n",
    "It also predicts the **uncertainty** in the state, denoted by the covariance matrix **P**:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{\\text{pred}} = \\mathbf{A} \\cdot \\mathbf{P} \\cdot \\mathbf{A}^T + \\mathbf{R}\n",
    "$$\n",
    "\n",
    "- **Q**: Process noise covariance (uncertainty from wheel encoders).\n",
    "\n",
    "**B. Update Step**\n",
    "\n",
    "When the camera provides a measurement **Z**, the Kalman Filter updates the state and uncertainty:\n",
    "\n",
    "1. Compute the Kalman gain **K**, which determines how much weight to give to the measurement:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\mathbf{P}_{\\text{pred}} \\cdot \\mathbf{H}^T \\cdot \\left(\\mathbf{H} \\cdot \\mathbf{P}_{\\text{pred}} \\cdot \\mathbf{H}^T + \\mathbf{Q}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "- **H**: Measurement matrix (maps state to camera readings).\n",
    "- **R**: Measurement noise covariance (uncertainty in camera data).\n",
    "\n",
    "2. Update the state using the measurement:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{update}} = \\mathbf{E}_{\\text{pred}} + \\mathbf{K} \\cdot \\left(\\mathbf{Z} - \\mathbf{H} \\cdot \\mathbf{E}_{\\text{pred}}\\right)\n",
    "$$\n",
    "\n",
    "3. Update the uncertainty:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{\\text{update}} = (\\mathbf{I} - \\mathbf{K} \\cdot \\mathbf{H}) \\cdot \\mathbf{P}_{\\text{pred}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Behavior in the Control Loop**\n",
    "\n",
    "1. **Prediction Always Happens**:\n",
    "    * The prediction step runs at every iteration using odometry.\n",
    "    * This ensures the robot always has an estimated position, even if the camera data is unavailable.\n",
    "\n",
    "2. **Update Happens When the Camera Sees the Robot**:\n",
    "    * If the camera detects the robot, the Kalman Filter corrects its prediction using the more accurate camera measurement.\n",
    "\n",
    "3. **Handling Obstructions**:\n",
    "    * If the camera is obstructed, the filter relies solely on the prediction step until the camera resumes detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning of Parameters\n",
    "The following section explains how the various parameters used throughout the project where selected and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Obstacle Detection Threshold**: the Thymio robot features 5 horizontal **proximity sensors** it its front part (see Thyimio cheat sheet snippet) that can be used to detect obstacles using infrared technology. The range of values that the sensors return is [0, 4300] and the updates come at a frequency of 10Hz.\n",
    "    <p align=\"center\">\n",
    "        <img src=\"images/thymio_cheat_sheet1.png\" width=\"800\">\n",
    "    </p>\n",
    "    It is possible to map the readings from the proximity sensors to the real world distance measurements in order to tune the threshold for activating the local avoidance routine. In order to do so, many measurements were taken with an obstacle at a known distance, for multiple distances, and then a linear interpolation allowed to find the function of the form\n",
    "\n",
    "    $f(\\text{sensor\\_reading}) = \\text{distance} = \\alpha \\cdot \\text{sensor\\_reading} + \\beta$\n",
    "\n",
    "    that maps sensor readings to obstacle distance.  \n",
    "    The mapping has been calculated to be: \n",
    "    \n",
    "    $distance = n \\cdot sensor\\_reading + m$\n",
    "\n",
    "    The following table shows the measurements that allowed to reconstruct the mapping:\n",
    "\n",
    "    | Distance | Sensor Reading (average value) |\n",
    "    | --- | ---: |\n",
    "    | 5 | 12343 |\n",
    "    | 10 | 123414 |\n",
    "    | 20 | 324123 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "The next section presents the runnable cells that allow the system to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First step, all the classes that have been explained above need to be imported.\n",
    "In order to be sure that all the python packages are correctly installed and ready to use, it is suggested to run <code>pip install -r requirements.txt</code> to get all of them. It should be noted that the <code>pupil-apriltags</code> package may give some problems with python versions higer than 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the classes in the package\n",
    "from classes.vision import Vision\n",
    "from classes.globalPlanning import GlobalPlanning\n",
    "from classes.thymioControl import ThymioControl\n",
    "from classes.localPlanning import LocalPlanning\n",
    "from classes.kalman import Kalman\n",
    "from classes.plotter import Plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install the <code>tdmclient</code> package, used for connecting and comunicating with the Thymio. The control from the notebook is made possible by the import of <code>tdmclient.notebook</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tdmclient in c:\\users\\feder\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.21)\n",
      "Requirement already satisfied: zeroconf in c:\\users\\feder\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tdmclient) (0.136.2)\n",
      "Requirement already satisfied: websockets in c:\\users\\feder\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tdmclient) (14.1)\n",
      "Requirement already satisfied: ifaddr>=0.1.7 in c:\\users\\feder\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zeroconf->tdmclient) (0.2.0)\n",
      "Requirement already satisfied: async-timeout>=3.0.0 in c:\\users\\feder\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zeroconf->tdmclient) (5.0.1)\n"
     ]
    },
    {
     "ename": "NodeLockError",
     "evalue": "Node lock error (current status: busy)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNodeLockError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import the TDMClient module\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtdmclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m tdmclient\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\feder\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\notebook\\private.py:112\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, password, debug, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m client \u001b[38;5;241m=\u001b[39m ClientAsync(zeroconf\u001b[38;5;241m=\u001b[39mzeroconf, zeroconf_all\u001b[38;5;241m=\u001b[39mzeroconf_all,\n\u001b[0;32m    108\u001b[0m                      tdm_addr\u001b[38;5;241m=\u001b[39mtdm_addr, tdm_port\u001b[38;5;241m=\u001b[39mtdm_port, tdm_ws\u001b[38;5;241m=\u001b[39mtdm_ws,\n\u001b[0;32m    109\u001b[0m                      password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[0;32m    110\u001b[0m                      debug\u001b[38;5;241m=\u001b[39mdebug)\n\u001b[0;32m    111\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mwait_for_node(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mlock()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _interactive_console\n\u001b[0;32m    115\u001b[0m _interactive_console \u001b[38;5;241m=\u001b[39m TDMConsole(local_var\u001b[38;5;241m=\u001b[39mget_ipython()\u001b[38;5;241m.\u001b[39muser_ns)\n",
      "File \u001b[1;32mc:\\Users\\feder\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdmclient\\clientasyncnode.py:61\u001b[0m, in \u001b[0;36mClientAsyncNode.lock\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_node()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NodeLockError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mNodeLockError\u001b[0m: Node lock error (current status: busy)"
     ]
    }
   ],
   "source": [
    "# pip install tdmclient\n",
    "!pip3 install tdmclient --upgrade\n",
    "\n",
    "# import the TDMClient module\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TdmClient Functions\n",
    "The following functions are used for modifying the variables in the Thymio, by comunicating through the tdmclient.\n",
    "Specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "def motor_go(left,right):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = left\n",
    "    motor_right_target = right\n",
    "    \n",
    "@tdmclient.notebook.sync_var\n",
    "def motor_stop():\n",
    "    global motor_left_target,motor_right_target\n",
    "    motor_left_target = 0\n",
    "    motor_right_target = 0\n",
    "    \n",
    "@tdmclient.notebook.sync_var\n",
    "def sensor_data():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal.copy()\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_off():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 0, 0]\n",
    "    leds_bottom_left = [0, 0, 0]\n",
    "    leds_bottom_right = [0, 0, 0]\n",
    "    leds_circle = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "def leds_go_crazy():\n",
    "    for i in range(8):\n",
    "        leds_1()\n",
    "        time.sleep(0.1)\n",
    "        leds_2()\n",
    "        time.sleep(0.1)\n",
    "        leds_3()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_1():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [32, 0, 0]\n",
    "    leds_bottom_left = [0, 32, 0]\n",
    "    leds_bottom_right = [0, 0, 32]\n",
    "    leds_circle = [32, 32, 32, 32, 0, 0, 0, 0]\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_2():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 32, 0]\n",
    "    leds_bottom_left = [0, 0, 32]\n",
    "    leds_bottom_right = [32, 0, 0]\n",
    "    leds_circle = [0, 0, 0, 0, 32, 32, 32, 32]\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def leds_3():\n",
    "    global leds_top, leds_bottom_left, leds_bottom_right, leds_circle\n",
    "    leds_top = [0, 0, 32]\n",
    "    leds_bottom_left = [32, 0, 0]\n",
    "    leds_bottom_right = [0, 32, 0]\n",
    "    leds_circle = [32, 0, 32, 0, 32, 0, 32, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GlobalPlanning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m kalman_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# initialize objects\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m globalPlanning \u001b[38;5;241m=\u001b[39m \u001b[43mGlobalPlanning\u001b[49m()\n\u001b[0;32m      7\u001b[0m localPlanning \u001b[38;5;241m=\u001b[39m LocalPlanning()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m Kalman()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GlobalPlanning' is not defined"
     ]
    }
   ],
   "source": [
    "x_trajectory = []\n",
    "y_trajectory = []\n",
    "kalman_positions = []\n",
    "\n",
    "# initialize objects\n",
    "globalPlanning = GlobalPlanning()\n",
    "localPlanning = LocalPlanning()\n",
    "filter = Kalman()\n",
    "thymio = ThymioControl()\n",
    "plotter = Plotter()\n",
    "\n",
    "dt = 0.15\n",
    "\n",
    "# create a Vision object\n",
    "image_path3 = \"images/IMG_7028.jpeg\"\n",
    "vision = Vision(fps=3,target_height=200, default_image_path=image_path3)\n",
    "\n",
    "iter = 1\n",
    "\n",
    "goal_pos = [0, 0]\n",
    "\n",
    "goal = False\n",
    "\n",
    "thymio.set_timestep(dt)\n",
    "\n",
    "leds_off()\n",
    "\n",
    "while not goal:\n",
    "    vision.update_image(live=False)\n",
    "    position = vision.getStart()\n",
    "    angle = vision.getAngle()\n",
    "\n",
    "    if (iter == 1):\n",
    "        print(\"First iteration\")\n",
    "        print(\"Getting map and Thymio position from camera\")\n",
    "        vision.update_image()\n",
    "        map = vision.getMatrix()\n",
    "        thymio.set_pose(position.copy(), angle)\n",
    "\n",
    "        if (position is None):\n",
    "            print(\"Error: camera obstructed in the first iteration\")\n",
    "            exit()\n",
    "                  \n",
    "        print(\"Computing path\")\n",
    "        print(\"Map shape: \", map.shape)\n",
    "        goal_pos = vision.getGoal()\n",
    "        path = globalPlanning.dijkstra(map, position, goal_pos)\n",
    "        thymio.set_path(path)\n",
    "        plotter.set_map(map, position, goal_pos)\n",
    "        plotter.plot_path(thymio.get_path_cells())\n",
    "        print(\"Path: \", thymio.get_path_cells())\n",
    "        x_trajectory.append(position[0])\n",
    "        y_trajectory.append(position[1])\n",
    "        filter.initialize_position(thymio.get_position()[0], thymio.get_position()[1], angle)\n",
    "        filter.set_lastKalman_time()\n",
    "\n",
    "    # check if camera is obstructed\n",
    "    if position is not None:\n",
    "        thymio.update_pose(position, angle)\n",
    "        print(\"Camera not obstructed, getting position from camera\")\n",
    "        x_trajectory.append(float(position[0]))\n",
    "        y_trajectory.append(float(position[1]))\n",
    "        # camera measuerement that will then be used for the Kalman filter\n",
    "        measurement = np.array([thymio.get_position()[0], thymio.get_position()[1], angle])\n",
    "        filter.kalman_update(measurement)\n",
    "        print(\"Kalman update: \", filter.get_state())\n",
    "        \n",
    "        # check if the robot has been kidnapped\n",
    "        if thymio.amIKidnapped():\n",
    "            print(\"Kidnapping detected\")\n",
    "\n",
    "            # tmdclient function to stop the motors\n",
    "            motor_stop()\n",
    "\n",
    "            # update the map with new robot position\n",
    "            time.sleep(2)\n",
    "            vision.update_image()\n",
    "            map = vision.getMatrix()\n",
    "            position = vision.getStart()\n",
    "            angle = vision.getAngle()\n",
    "\n",
    "            # new path planning\n",
    "            path = globalPlanning.dijkstra(map, position, goal_pos)\n",
    "            thymio.set_path(path)\n",
    "            plotter.plot_path(thymio.get_path_cells())\n",
    "            x_trajectory.append(position[0])\n",
    "            y_trajectory.append(position[1])\n",
    "            filter.initialize_position(thymio.get_position()[0], thymio.get_position()[1], angle)\n",
    "            filter.set_lastKalman_time()\n",
    "\n",
    "        if vision.getGoal() != goal_pos:\n",
    "            print(\"Goal changed\")\n",
    "            goal_pos = vision.getGoal()\n",
    "            path = globalPlanning.dijkstra(map, position, goal_pos)\n",
    "            thymio.set_path(path)\n",
    "            plotter.plot_path(thymio.get_path_cells())\n",
    "    \n",
    "    else:\n",
    "        print(\"Camera obstructed\")\n",
    "\n",
    "    # get the state from the Kalman filter\n",
    "    # the result will depend on the mode of the filter\n",
    "    x, y, angle = filter.get_state()\n",
    "    kalman_position = [x, y]\n",
    "    kalman_positions.append(position)\n",
    "    print(\"Kalman position used: \", kalman_position)\n",
    "\n",
    "    # check if the robot is detecting an obstacle\n",
    "    # tmclient function to get the proximity sensors\n",
    "    prox = sensor_data()\n",
    "    if (localPlanning.is_obstacle_avoidance(prox)):\n",
    "        # move with local planning until the robot is not back on the path\n",
    "        wl, wr = localPlanning.obstacle_avoidance(prox)\n",
    "        v, w = thymio.inverseDifferentialDrive(wl, wr)\n",
    "    else:\n",
    "        # move with global planning\n",
    "        v, w, wl, wr, goal = thymio.move(kalman_position, angle)\n",
    "\n",
    "    # update the Kalman filter\n",
    "    filter.kalman_prediction(wl, wr)\n",
    "\n",
    "    # tmdclient function to move the motors\n",
    "    wl = int(wl)\n",
    "    wr = int(wr)\n",
    "\n",
    "    print(\"Kalman pos 0: \", kalman_position[0])\n",
    "    print(\"Kalman pos 1: \", kalman_position[1])\n",
    "    print(\"v, w \", v, w)\n",
    "    print(\"angle: \", angle)\n",
    "    print(\"New angle: \", (angle + w*dt) % (2 * math.pi))\n",
    "\n",
    "\n",
    "    print(\"Trajectory: \", x_trajectory, y_trajectory)\n",
    "    plotter.plot_trajectory(x_trajectory, y_trajectory)\n",
    "\n",
    "    motor_go(wl, wr)\n",
    "    \n",
    "    # sleep for a while\n",
    "    iter += 1\n",
    "    time.sleep(dt)\n",
    "\n",
    "motor_stop()\n",
    "leds_go_crazy()\n",
    "leds_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
